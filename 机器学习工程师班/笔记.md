# 第一课

## 1.微积分选讲
### 极限
思想： 极限 -> 逼近 -> 导数
 求导法则：
 * 链式
 * 加法
 * 乘法
 * 反函数

函数的高阶导数

可导一定连续，
连续不一定可导。

### 微分与泰勒级数
用多项式逼近的方式描述 高阶导数 -- 泰勒级数

### 积分与微积分基本定理
* 单变量的黎曼积分


### 牛顿法
* 寻找局部极值
* 必须给出一个初始值X0.
* 二阶逼近。
* 对局部凸函数找到极小值，对凹函数找到极大值，对局部不凹不凸的函数找到靶点。

参考资料：
《数学分析简明教程》， 史济怀
《简明微积分》
《微积分讲义》 陈省身

p142:2,3,7,8
p143:3,4,6

## 2. 线性代数选讲

基石线性空间里的一组向量。

### 线性映射与矩阵

### 矩阵变换与特征值

### 奇异值分解

### 应用举例：PCA
目标： 降维

# 第2课 概率论与凸优化
## 1概率论
### 概率空间与随机变量
* 概率空间
	- Ω：样本空间。
	- F：事件（Ω的子集构成）
	- P：测度（事件的概率）

* 随机变量

### 贝叶斯公式
条件概率：
P(A|B) = P(B|A)P(A)/P(B)

### 随机变量的特征函数

### 两大基本定理

### 参数估计
x1=10;x2=2.23
print(type(x1), type(2))

## 2.凸优化

### 凸优化问题

### 凸集合和凸函数

### 保凸运算

### 共轭函数

### 带边界优化的对偶问题
### KKT条件


# 第三课 回归模型与应用

* Unsupervised Learning
* Supervised Learning
* Reinforcement Learning

## 1. 线性回归
### 定义

### 损失函数


* 梯度下降：
	 逐步最小化损失函数的过程。

* 梯度下降与学习率

* underfitting vs overfitting
回归于欠/过拟合

### 过拟合与正则化
* 正则化：
	- 控制参数幅度，不让模型超出控制
	- 限制参数搜索空间。


## 2.逻辑斯蒂回归
### 定义
* 命名对离散值预测，为啥叫回归
* 归功于sigmoid函数

### 损失函数

### 梯度下降与正则化

* 二分类与多分类
*

## 3.工程应用试验







